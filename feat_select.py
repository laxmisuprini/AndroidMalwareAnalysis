
from sklearn.ensemble import RandomForestClassifier
import numpy as np
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
import read
from sklearn.svm import SVC
import tkinter as tk
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg



def select_features_random_forest(x_train, x_test, y_train, y_test):
    
    
    print (x_train.shape, y_train.shape)
    print (x_test.shape, y_test.shape)
    random_forest_classifier = RandomForestClassifier(max_features='sqrt', n_estimators=1000)

    random_forest_classifier.fit(x_train, y_train)
    importances = random_forest_classifier.feature_importances_
    #print(random_forest_classifier.best_score_)

    indices = np.argsort(importances)[::-1]

    std = np.std([tree.feature_importances_
                  for tree in random_forest_classifier.estimators_], axis=0)

    # Print the feature ranking
    print("\nFeature ranking (ordered DESC) using random forest classifier:")

    for f in range(x.shape[1]):
        print("%d. feature %d (%f)" %
              (f + 1, indices[f], importances[indices[f]]))


     


    root = tk.Tk()    


    #plt.show() 
     

    figure1 = plt.figure(figsize=(6,5), dpi=100)
    plt.subplot(111)
    bar1 = FigureCanvasTkAgg(figure1, root)
    bar1.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH)
    plt.title("Features importance due to random forest classifier")
    plt.bar(range(x.shape[1]), importances[indices],
            color="b", yerr=std[indices], align="center")
    plt.xticks(range(x.shape[1]), indices)
    plt.xlim([-1, x.shape[1]])
    root.mainloop()
    y_pred = random_forest_classifier.predict(x_test)
    print ("Accuracy is %f." % accuracy_score(y_test, y_pred))
    print ("precision is %f." % precision_score(y_test,y_pred) ) 
    print ("recall is %f."% recall_score(y_test,y_pred))
    print ("F1 score is %f."% f1_score(y_test,y_pred))
    #plt.show()

'''    figure2 = plt.figure(figsize=(5,4), dpi=100)
    plt.subplot(111)
    line2 = FigureCanvasTkAgg(figure2, root)
    line2.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH)
    plt.title("Features importance due to extra trees classifier")
    plt.bar(range(x.shape[1]), importances[indices],
            color="g", yerr=std[indices], align="center")
    plt.xticks(range(x.shape[1]), indices)
    plt.xlim([-1, x.shape[1]])'''

    



'''extra_trees = ExtraTreesClassifier(n_estimators=100)
    # ET Fit
    extra_trees.fit(x_train, y_train)
    # ET Predict
    y_predicted = extra_trees.predict(x_test)
    # ET Matrices
    print("ET Evaluation parameters:")
    #print_metrices_out(y_predicted, y_test)  
    y_pt = extra_trees.predict(x_test)
    print ("Accuracy is %f." % accuracy_score(y_test, y_pt))
    

    importances = extra_trees.feature_importances_

    indices = np.argsort(importances)[::-1]

    std = np.std([tree.feature_importances_
                  for tree in extra_trees.estimators_], axis=0)

    # Print the feature ranking
    print("\nFeature ranking (ordered DESC) using extra trees classifier:")

    for f in range(x.shape[1]):
        print("%d. feature %d (%f)" %
              (f + 1, indices[f], importances[indices[f]]))'''



def select_features_recursive_feature_elimination(x_train, x_test, y_train, y_test):
    model = LogisticRegression(solver='lbfgs')
    rfe = RFE(model, 4)
    fit = rfe.fit(x_train, y_train)
    print('\nRFE chose the the top 4 features: ')
    print('Numbers Features: ' + str(fit.n_features_))
    print('Selected Features: ' + str(fit.support_))
    print('Feature Ranking: ' + str(fit.ranking_))
    y_predicted = fit.predict(x_test)
    print ("Accuracy is %f." % accuracy_score(y_test, y_predicted))
    print ("precision is %f." % precision_score(y_test,y_predicted) ) 
    print ("recall is %f."% recall_score(y_test,y_predicted))
    print ("F1 score is %f."% f1_score(y_test,y_predicted))
    '''
def train_extra_trees(x_train, y_train, x_test, y_test):
    
    extra_trees = ExtraTreesClassifier(n_estimators=100)
    # ET Fit
    extra_trees.fit(x_train, y_train)
    # ET Predict
    y_predicted = extra_trees.predict(x_test)
    # ET Matrices
    print("ET Evaluation parameters:")
    print_metrices_out(y_predicted, y_test)  
    

    importances = extra_trees.feature_importances_

    indices = np.argsort(importances)[::-1]

    std = np.std([tree.feature_importances_
                  for tree in extra_trees.estimators_], axis=0)

    # Print the feature ranking
    print("\nFeature ranking (ordered DESC) using extra trees classifier:")

    for f in range(x.shape[1]):
        print("%d. feature %d (%f)" %
              (f + 1, indices[f], importances[indices[f]]))

    plt.figure()
    plt.title("Features importance due to extra trees classifier")
    plt.bar(range(x.shape[1]), importances[indices],
            color="g", yerr=std[indices], align="center")
    plt.xticks(range(x.shape[1]), indices)
    plt.xlim([-1, x.shape[1]])
    plt.show() ''' 

def train_svm(x_train, y_train, x_test, y_test):
    print("\n-------------SVM Model-------------")
    model = SVC(gamma='scale')
    # SVM Fit
    model.fit(x_train, y_train)
    # SVM Predict
    y_predicted = model.predict(x_test) 
    print ("Accuracy is %f." % accuracy_score(y_test, y_predicted))  
    print ("precision is %f." % precision_score(y_test,y_predicted) ) 
    print ("recall is %f."% recall_score(y_test,y_predicted))
    print ("F1 score is %f."% f1_score(y_test,y_predicted))



x,y=read.read_dat()
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)
select_features_random_forest(x_train, x_test, y_train, y_test)
select_features_recursive_feature_elimination(x_train, x_test, y_train, y_test)    
train_svm(x_train, y_train, x_test, y_test)